AA <- waffle(nyc, rows=7, size=0.5,title="NLU Performance and Transfer")
iron(AA)
AA <- waffle(nyc, rows=7, size=0.5, colors=c("#1A1F2B", "#30395C", "#4A6491", "#85A5CC"), title="NLU Performance and Transfer")
iron(AA)
AA <- waffle(nyc, rows=7, size=0.5, colors=c("#225378", "#EB7F00", "#F3FFE2", "#1695A3"), title="NLU Performance and Transfer")
iron(AA)
nyc2<- c('Transfer 77.9%'=77.9, 'Containment 9%'=10.6, 'Fed/State 6.6%'=6.6, 'City Agency 6.3%'=6.3)
nyc3<- c('Transfer 77.9%'=77.9, 'Containment 9.7%'=9.7, 'Fed/State Agency 5.9%'=5.9, 'City Agency 5.8%'=5.8, 'Other 0.8%'=0.8)
BB <- waffle(nyc2, rows=7, size=0.5,
colors=c("#004358", "#1F8A70", "#BEDB39", "#FFE11A", "#FD7400" ), title="NLU Performance & Transfer")
iron(BB)
BB <- waffle(nyc2, rows=7, size=0.5,
colors=c("#004358", "#1F8A70", "#BEDB39", "#FFE11A", "#FD7400" ), title="NLU Performance & Transfer", )+
theme_wsj() +
scale_x_continuous(breaks=NULL) +
scale_y_continuous(breaks=NULL)
BB <- waffle(nyc2, rows=7, size=0.5,
colors=c("#004358", "#1F8A70", "#BEDB39", "#FFE11A", "#FD7400" ), title="NLU Performance & Transfer", )
iron(BB)
BB <- waffle(nyc2, rows=10, size=2,
colors=c("#004358", "#1F8A70", "#BEDB39", "#FFE11A", "#FD7400" ), title="NLU Performance & Transfer")
iron(BB)
BB <- waffle(nyc2, rows=10, size=5,
colors=c("#004358", "#1F8A70", "#BEDB39", "#FFE11A", "#FD7400" ), title="NLU Performance & Transfer")
iron(BB)
BB <- waffle(nyc2, rows=10, size=0.5,
colors=c("#004358", "#1F8A70", "#BEDB39", "#FFE11A", "#FD7400" ), title="NLU Performance & Transfer")
iron(BB)
BB <- waffle(nyc2, rows=10, size=0.8,
colors=c("#004358", "#1F8A70", "#BEDB39", "#FFE11A", "#FD7400" ), title="NLU Performance & Transfer")
iron(BB)
BB <- waffle(nyc2, rows=10, size=0.8,
colors=c("#004358", "#1F8A70", "#BEDB39", "#FFE11A", "#FD7400" ), title="NLU Performance & Transfer") + theme(legend.title = element_text(colour="blue", face="bold", size=14))
iron(BB)
BB <- waffle(nyc2, rows=10, size=0.8,
colors=c("#004358", "#1F8A70", "#BEDB39", "#FFE11A", "#FD7400" ), title="NLU Performance & Transfer") + theme(legend.text = element_text(colour="blue", face="bold", size=16))
BB <- waffle(nyc2, rows=10, size=0.8,
colors=c("#004358", "#1F8A70", "#BEDB39", "#FFE11A", "#FD7400" ), title="NLU Performance & Transfer") + theme(legend.text = element_text(colour="blue", face="bold", size=16))
iron(BB)
BB <- waffle(nyc2, rows=10, size=0.8,
colors=c("#004358", "#1F8A70", "#BEDB39", "#FFE11A", "#FD7400" ), title="NLU Performance & Transfer") + theme(legend.text = element_text(face="bold", size=14))
iron(BB)
BB <- waffle(nyc2, rows=10, size=0.8,
colors=c("#004358", "#1F8A70", "#BEDB39", "#FFE11A", "#FD7400" ), title="NLU Performance & Transfer") + theme(legend.text = element_text(face="bold", size=14))
library(gcookbook)
tophit<- tophitters2001[1:25, ]
View(tophit)
ggplot(tophit, aes(x=avg,y=name)) + geom_point()
tophit[, c("name", "lg", "avg")]
ggplot(tophit, aes(x=avg, y=reorder(name, avg))) + geom_point(size=5) + theme_bw() + theme(panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank() panel.grid.major.y = element_line(colour="grey60", linetype="dashed"))
ggplot( tophit, aes( x = avg, y = reorder( name, avg))) + geom_point( size = 5) + # Use a larger dot theme_bw() + theme( panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank(), panel.grid.major.y = element_line( colour ="grey60", linetype ="dashed"))
ggplot( tophit, aes( x = avg, y = reorder( name, avg))) + geom_point( size = 5) + # Use a larger dot theme_bw() + theme( panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank(), panel.grid.major.y = element_line( colour ="grey60", linetype ="dashed"))
library( gcookbook) # For the data set tophit <- tophitters2001[ 1: 25, ] # Take the top 25 from the tophitters data set ggplot( tophit, aes( x = avg, y = name)) + geom_point()
library(gcookbook) # For the data set tophit
tophit <- tophitters2001[ 1: 25, ]
ggplot( tophit, aes( x = avg, y = name)) + geom_point()
library("ggplot2", lib.loc="C:/Program Files/R/R-3.1.3/library")
ggplot( tophit, aes( x = avg, y = name)) + geom_point()
tophit[, c("name", "lg", "avg")]
ggplot(tophit, aes( x = avg, y = reorder( name, avg))) + geom_point( size = 5) + # Use a larger dot theme_bw() + theme( panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank(), panel.grid.major.y = element_line( colour ="grey60", linetype ="dashed"))
ggplot(tophit, aes(x = avg, y = reorder(name, avg))) + geom_point(size = 5) + theme_bw() + theme(panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank(), panel.grid.major.y = element_line( colour ="grey60", linetype ="dashed"))
ggplot(tophit, aes(x = avg, y = name)) + geom_segment( aes( yend = name), xend = 0, colour ="grey50") + geom_point( size = 5, aes(colour = lg)) + scale_colour_brewer(palette ="Set1", limits = c("NL","AL")) + theme_bw() + theme(panel.grid.major.y = element_blank()
getwd()
library(waffle)
require(ggplot2)
require(gtable)
require(grid)
nyc<- c('Transfer 77.6%'=77.6, 'Containment 9.6%'=9.6, 'Fed/State Agency 5.9%'=5.9, 'Other City Agency 2.4%'=2.4, 'Other 1.0%'=1, 'NYCHA 0.65'=0.6)
A <- waffle(nyc, rows=10, size=0.8,
colors=c("#004358", "#1F8A70", "#BEDB39", "#FFE11A", "#FD7400" ), title="NLU Performance & Transfer") + theme(legend.text = element_text(face="bold", size=14))
iron(A)
nyc<- c('Transfer 77.6%'=77.6, 'Containment 9.6%'=9.6, 'Fed/State Agency 5.9%'=5.9, 'Other City Agency 6.2%'=6.2, 'Other 1.0%'=1)
A <- waffle(nyc, rows=10, size=0.8,
colors=c("#004358", "#1F8A70", "#BEDB39", "#FFE11A", "#FD7400" ), title="NLU Performance & Transfer") + theme(legend.text = element_text(face="bold", size=14))
iron(A)
nyc<- c('Transfer 77.6%'=77, 'Containment 9.6%'=10, 'Fed/State Agency 5.9%'=6, 'Other City Agency 6.2%'=6, 'Other 1.0%'=1)
A <- waffle(nyc, rows=10, size=0.8,
colors=c("#004358", "#1F8A70", "#BEDB39", "#FFE11A", "#FD7400" ), title="NLU Performance & Transfer") + theme(legend.text = element_text(face="bold", size=14))
iron(A)
A <- waffle(nyc, rows=10, size=0.8,
colors=c("#3182bd", "#6baed6", "#9ecae1", "#e6550d", "#fd8d3c" ), title="NLU Performance & Transfer") + theme(legend.text = element_text(face="bold", size=14))
iron(A)
A <- waffle(nyc, rows=10, size=0.8,
colors=c("#6baed6", "#fd8d3c", "#9ecae1", "#9e9ac8", "#969696" ), title="NLU Performance & Transfer") + theme(legend.text = element_text(face="bold", size=14))
iron(A)
A <- waffle(nyc, rows=5, size=0.8,
colors=c("#6baed6", "#fd8d3c", "#9ecae1", "#9e9ac8", "#969696" ), title="NLU Performance & Transfer") + theme(legend.text = element_text(face="bold", size=14))
iron(A)
A <- waffle(nyc, rows=8, size=0.8,
colors=c("#6baed6", "#fd8d3c", "#9ecae1", "#9e9ac8", "#969696" ), title="NLU Performance & Transfer") + theme(legend.text = element_text(face="bold", size=14))
iron(A)
A <- waffle(nyc, rows=5, size=0.8,
colors=c("#6baed6", "#fd8d3c", "#9ecae1", "#9e9ac8", "#969696" ), title="NLU Performance & Transfer") + theme(legend.text = element_text(size=12))
iron(A)
A <- waffle(nyc, rows=5, size=1,
colors=c("#6baed6", "#fd8d3c", "#9ecae1", "#9e9ac8", "#969696" ), title="NLU Performance & Transfer") + theme(legend.text = element_text(size=12))
iron(A)
A <- waffle(nyc, rows=5, size=0.5,
colors=c("#6baed6", "#fd8d3c", "#9ecae1", "#9e9ac8", "#969696" ), title="NLU Performance & Transfer") + theme(legend.text = element_text(size=12))
iron(A)
A <- waffle(nyc, rows=5, size=0.8,
colors=c("#6baed6", "#fd8d3c", "#9ecae1", "#9e9ac8", "#969696" ), title="NLU Performance & Transfer") + theme(legend.text = element_text(size=12))
iron(A)
library(devtools)
install_github("leeper/slopegraph")
library(httr)
set_config(
use_proxy(url="bcpxy.nycnet", port=8080, username="timothymartin76",password="chichewa1")
)
install_github("leeper/slopegraph")
require(slopegraph)
getwd()
require(arules)
install.packages("C:/Users/timartin/Downloads/arules_1.3-1.zip", repos = NULL)
library("arules", lib.loc="C:/Program Files/R/R-3.1.3/library")
install.packages("devtools")
library("party")
install.packages("party")
require(party)
str(iris)
iris_ctree <- ctree(Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, data=iris)
print(iris_ctree)
plot(iris_ctree)
what<- str(iris)
library(ISLR)
install.packages("ISLR")
require(ISLR)
install.packages("tree")
require(tree)
attach(Carseats)
head(Carseats)
ca<- Carseats
View(ca)
range(Sales)
High = ifelse(Sales>=8, "Yes", "No")
length(High)
dim(Carseats)
Carseats = data.fram(Carseats, High)
Carseats = data.frame(Carseats, High)
View(Carseats)
names(Carseats)
Carseats = Carseasts[,-1]
View(Carseats)
Carseats = Carseats[,-1]
View(Carseats)
set.seed(2)
train= sample(1:nrow(Carseats), nrow(Carseats)/2)
test = -train
training_data = Carseats[train,]
testing_data = Carseats[test, ]
testing_High = High[test]
tree_model = tree(High~., training_data)
plot(tree_model)
text(tree_model, pretty = 0)
tree_pred = predict(tree_model, testing_data, type = "class")
mean(tree_pred !=testing_High)
set.seed(3)
cv_tree = cv.tree(tree_model, FUN = prune.misclass)
names(cv_tree)
plot(cv_tree$size, cv_tree$dev)
plot(cv_tree$size, cv_tree$dev, type = "b")
pruned_model = prune.misclass(tree_model, best = 9)
plot(pruned_model)
text(pruned_model, pretty = 0)
tree_pred = predict(pruned_model, testing_data, type = "class")
mean(tree_pred != testing_High)
setwd("C:/SL_Decision_Tree")
mydata<- read.csv("SL_data.csv", header=T, sep=',')
View(mydata)
head(mydata)
range(Service.Level)
range(Service.Level)
range(mydata$Service.Level)
High = ifelse(Service.Level >=80, "Yes", "No")
High = ifelse(mydata$Service.Level >=80, "Yes", "No")
mydata = data.frame(mydata, High)
View(mydata)
length(High)
set.seed(2)
mydata = mydata[,-1]
View(mydata)
set.seed
set.seed(2)
train = sample(1:nrow(mydata), nrow(mydata)/2)
test = -train
training_data = mydata[train,]
testing_data = mydata[test, ]
testing_High = High[test]
tree_model = tree(High~., training_data)
plot(tree_model)
text(tree_model, pretty = 0)
tree_pred = predict(tree_model, testing_data, type = "class")
mean(tree_pred != testing_High)
set.seed(3)
cv_tree = cv.tree(tree_model, FUN = prune.misclass)
names(cv_tree)
plot(cv_tree$size, cv_tree$dev)
plot(cv_tree$size, cv_tree$dev, type = "b")
pruned_model = prune.misclass(tree_model, best = 2)
plot(pruned_model)
text(pruned_model, pretty = 0)
pruned_model = prune.misclass(tree_model, best = 5)
plot(pruned_model)
text(pruned_model, pretty = 0)
tree_pred = predict(pruned_model, testing_data, type = "class")
mean(tree_pred != testing_High)
pruned_model = prune.misclass(tree_model, best = 8)
plot(pruned_model)
text(pruned_model, pretty = 0)
getwd()
mydata<- read.csv("SL_2.csv", header=T, sep=',')
View(mydata)
require(tree)
range(mydata$Service.Level)
range(Service.Level)
range(mydata$Service.Level)
head(mydata)
range(mydata$Service.Level)
##Create cateogorical variables of SL >80%
High = ifelse(mydata$Service.Level >=80, "Yes", "No")
##Append "High" to mydata
mydata = data.frame(mydata, High)
View(mydata)
length(High)
View(mydata)
##Remove "Service.Level" field
mydata = mydata[,-1]
View(mydata)
##Split data into testing and training
set.seed(2)
train = sample(1:nrow(mydata), nrow(mydata)/2)
test = -train
training_data = mydata[train,]
testing_data = mydata[test,]
testing_High = High[test]
##fit the tree model using the training data
tree_model = tree(High~., training_data)
##plot the tree use pretty = 0 for categorical variable
plot(tree_model)
text(tree_model, pretty = 0)
##check the model using the test data
tree_pred = predict(tree_model, testing_data, type ="class")
mean(tree_pred != testing_High)
set.seed(3)
cv_tree = cv.tree(tree_model, FUN = prune.misclass)
names(cv_tree)
##plot size vs error rate
plot(cv_tree$size, cv_tree$dev, type = "b")
##prune the tree - create model
prune_model = prune.misclass(tree_model, best = 6)
plot(pruned_model)
text(pruned_model, pretty = 0)
plot(prune_model)
text(prune_model, pretty = 0)
##Fitting Classification Tree Models
##to fit decision trees
require(tree)
##SL dataset
mydata<- read.csv("SL_data.csv", header=T, sep=',')
View(mydata)
##Create cateogorical variables of SL >80%
High = ifelse(mydata$Service.Level >=80, "Yes", "No")
##Append "High" to mydata
mydata = data.frame(mydata, High)
##Remove "Service.Level" field
mydata = mydata[,-1]
##Split data into testing and training
set.seed(2)
train = sample(1:nrow(mydata), nrow(mydata)/2)
test = -train
training_data = mydata[train,]
testing_data = mydata[test,]
testing_High = High[test]
##fit the tree model using the training data
tree_model = tree(High~., training_data)
##plot the tree use pretty = 0 for categorical variable
plot(tree_model)
text(tree_model, pretty = 0)
##check the model using the test data
tree_pred = predict(tree_model, testing_data, type ="class")
mean(tree_pred != testing_High) #6.1%
##Prune the tree
##Run cross validation to check where to prune
set.seed(3)
cv_tree = cv.tree(tree_model, FUN = prune.misclass)
names(cv_tree)
##plot size vs error rate shows where to prune tree
plot(cv_tree$size, cv_tree$dev, type = "b")
##prune the tree - create model
pruned_model = prune.misclass(tree_model, best = 6)
plot(pruned_model)
text(pruned_model, pretty = 0)
##check how model is doing - if pruned model better than unpruned
tree_pred = predict(pruned_model, testing_data, type = "class")
mean(tree_pred != testing_High)
##Fitting Classification Tree Models
##to fit decision trees
require(tree)
##SL dataset
mydata<- read.csv("SL_data.csv", header=T, sep=',')
View(mydata)
##Create cateogorical variables of SL >80%
High = ifelse(mydata$Service.Level >=80, "Yes", "No")
##Append "High" to mydata
mydata = data.frame(mydata, High)
##Remove "Service.Level" field
mydata = mydata[,-1]
##Split data into testing and training
set.seed(2)
train = sample(1:nrow(mydata), nrow(mydata)/2)
test = -train
training_data = mydata[train,]
testing_data = mydata[test,]
testing_High = High[test]
##fit the tree model using the training data
tree_model = tree(High~., training_data)
##plot the tree use pretty = 0 for categorical variable
plot(tree_model)
text(tree_model, pretty = 0)
##check the model using the test data
tree_pred = predict(tree_model, testing_data, type ="class")
mean(tree_pred != testing_High) #6.1%
##Prune the tree
##Run cross validation to check where to prune
set.seed(3)
cv_tree = cv.tree(tree_model, FUN = prune.misclass)
names(cv_tree)
##plot size vs error rate shows where to prune tree
plot(cv_tree$size, cv_tree$dev, type = "b")
##prune the tree - create model
pruned_model = prune.misclass(tree_model, best = 6)
plot(pruned_model)
text(pruned_model, pretty = 0)
##check how model is doing - if pruned model better than unpruned
tree_pred = predict(pruned_model, testing_data, type = "class")
mean(tree_pred != testing_High)
title(main="probability of not meeting SL")
title(cex.lab=0.75)
title(cex=0.6)
text(csx=0.6)
text(cex=0.6)
text(pruned_model, cex=0.5)
text(pruned_model, cex=0.5, pos=4)
##Fitting Classification Tree Models
##to fit decision trees
require(tree)
##SL dataset
mydata<- read.csv("SL_data.csv", header=T, sep=',')
View(mydata)
##Create cateogorical variables of SL >80%
High = ifelse(mydata$Service.Level >=80, "Yes", "No")
##Append "High" to mydata
mydata = data.frame(mydata, High)
##Remove "Service.Level" field
mydata = mydata[,-1]
##Split data into testing and training
set.seed(2)
train = sample(1:nrow(mydata), nrow(mydata)/2)
test = -train
training_data = mydata[train,]
testing_data = mydata[test,]
testing_High = High[test]
##fit the tree model using the training data
tree_model = tree(High~., training_data)
##plot the tree use pretty = 0 for categorical variable
plot(tree_model)
text(tree_model, pretty = 0)
##check the model using the test data
tree_pred = predict(tree_model, testing_data, type ="class")
mean(tree_pred != testing_High) #6.1%
##Prune the tree
##Run cross validation to check where to prune
set.seed(3)
cv_tree = cv.tree(tree_model, FUN = prune.misclass)
names(cv_tree)
##plot size vs error rate shows where to prune tree
plot(cv_tree$size, cv_tree$dev, type = "b")
##prune the tree - create model
pruned_model = prune.misclass(tree_model, best = 6)
plot(pruned_model)
text(pruned_model, pretty = 0)
title(main="Will we meet Service Level?")
##check how model is doing - if pruned model better than unpruned
tree_pred = predict(pruned_model, testing_data, type = "class")
mean(tree_pred != testing_High)
text(pruned_model, pretty = 0, cex=0.5)
##Fitting Classification Tree Models
##to fit decision trees
require(tree)
##SL dataset
mydata<- read.csv("SL_data.csv", header=T, sep=',')
View(mydata)
##Create cateogorical variables of SL >80%
High = ifelse(mydata$Service.Level >=80, "Yes", "No")
##Append "High" to mydata
mydata = data.frame(mydata, High)
##Remove "Service.Level" field
mydata = mydata[,-1]
##Split data into testing and training
set.seed(2)
train = sample(1:nrow(mydata), nrow(mydata)/2)
test = -train
training_data = mydata[train,]
testing_data = mydata[test,]
testing_High = High[test]
##fit the tree model using the training data
tree_model = tree(High~., training_data)
##plot the tree use pretty = 0 for categorical variable
plot(tree_model)
text(tree_model, pretty = 0)
##check the model using the test data
tree_pred = predict(tree_model, testing_data, type ="class")
mean(tree_pred != testing_High) #6.1%
##Prune the tree
##Run cross validation to check where to prune
set.seed(3)
cv_tree = cv.tree(tree_model, FUN = prune.misclass)
names(cv_tree)
##plot size vs error rate shows where to prune tree
plot(cv_tree$size, cv_tree$dev, type = "b")
##prune the tree - create model
pruned_model = prune.misclass(tree_model, best = 6)
plot(pruned_model)
text(pruned_model, pretty = 0, cex=0.7)
title(main="Will we meet Service Level?")
##check how model is doing - if pruned model better than unpruned
tree_pred = predict(pruned_model, testing_data, type = "class")
mean(tree_pred != testing_High)
##Fitting Classification Tree Models
##to fit decision trees
require(tree)
##SL dataset
mydata<- read.csv("SL_data.csv", header=T, sep=',')
View(mydata)
##Create cateogorical variables of SL >80%
High = ifelse(mydata$Service.Level >=80, "Yes", "No")
##Append "High" to mydata
mydata = data.frame(mydata, High)
##Remove "Service.Level" field
mydata = mydata[,-1]
##Split data into testing and training
set.seed(2)
train = sample(1:nrow(mydata), nrow(mydata)/2)
test = -train
training_data = mydata[train,]
testing_data = mydata[test,]
testing_High = High[test]
##fit the tree model using the training data
tree_model = tree(High~., training_data)
##plot the tree use pretty = 0 for categorical variable
plot(tree_model)
text(tree_model, pretty = 0)
##check the model using the test data
tree_pred = predict(tree_model, testing_data, type ="class")
mean(tree_pred != testing_High) #6.1%
##Prune the tree
##Run cross validation to check where to prune
set.seed(3)
cv_tree = cv.tree(tree_model, FUN = prune.misclass)
names(cv_tree)
##plot size vs error rate shows where to prune tree
plot(cv_tree$size, cv_tree$dev, type = "b")
##prune the tree - create model
pruned_model = prune.misclass(tree_model, best = 6)
plot(pruned_model)
text(pruned_model, pretty = 0, cex=0.9)
title(main="Will we meet Service Level?", cex=2)
##check how model is doing - if pruned model better than unpruned
tree_pred = predict(pruned_model, testing_data, type = "class")
mean(tree_pred != testing_High)
##Fitting Classification Tree Models
##to fit decision trees
require(tree)
##SL dataset
mydata<- read.csv("SL_data.csv", header=T, sep=',')
View(mydata)
##Create cateogorical variables of SL >80%
High = ifelse(mydata$Service.Level >=80, "Yes", "No")
##Append "High" to mydata
mydata = data.frame(mydata, High)
##Remove "Service.Level" field
mydata = mydata[,-1]
##Split data into testing and training
set.seed(2)
train = sample(1:nrow(mydata), nrow(mydata)/2)
test = -train
training_data = mydata[train,]
testing_data = mydata[test,]
testing_High = High[test]
##fit the tree model using the training data
tree_model = tree(High~., training_data)
##plot the tree use pretty = 0 for categorical variable
plot(tree_model)
text(tree_model, pretty = 0)
##check the model using the test data
tree_pred = predict(tree_model, testing_data, type ="class")
mean(tree_pred != testing_High) #6.1%
##Prune the tree
##Run cross validation to check where to prune
set.seed(3)
cv_tree = cv.tree(tree_model, FUN = prune.misclass)
names(cv_tree)
##plot size vs error rate shows where to prune tree
plot(cv_tree$size, cv_tree$dev, type = "b")
##prune the tree - create model
pruned_model = prune.misclass(tree_model, best = 6)
plot(pruned_model)
text(pruned_model, pretty = 0, cex=0.9)
title(main="Will we meet Service Level?")
##check how model is doing - if pruned model better than unpruned
tree_pred = predict(pruned_model, testing_data, type = "class")
mean(tree_pred != testing_High)
